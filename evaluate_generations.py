import os.path
from pathlib import Path
import argparse
from collections import defaultdict
import pickle

import datasets


from dataset_utils import *
from eval_utils import *
from visualization_utils import *

def load_if_exists(pickle_path):
    if os.path.exists(pickle_path):
        with open(pickle_path, 'rb') as f:
            data = pickle.load(f)
        print(f"Loaded cache from: {pickle_path}")
        return data
    else:
        return None


hostname = os.uname()[1]
if hostname == "PTB-09003439":
    hf_cache_dir = "/home/flowers-user/.cache/huggingface"
else:
    hf_cache_dir = os.environ["HF_HOME"]

parser = argparse.ArgumentParser()
parser.add_argument("--experiment-dir", type=str, help="Experiment directory")
parser.add_argument("--emb", action="store_true", help="Embeddings and embedding based metrics")
parser.add_argument("--ce", action="store_true", help="Compute perplexity")
parser.add_argument("--pol", action="store_true", help="Compute political lean")
parser.add_argument("--tox", action="store_true", help="Compute political lean")
parser.add_argument("--n-samples-to-show", type=int, default=0, help="Show n random samples")
parser.add_argument("--visualize-datasets", action="store_true", help="Visualize the datasets")
parser.add_argument("--participant", type=str, default="part_0")
args = parser.parse_args()
print(args)

# load and encode AI datasets
print(f"Participant: {args.participant}")
experiment_dir = Path(args.experiment_dir)

if len(list(experiment_dir.glob(f"*/{args.participant}"))) == 0:
    raise ValueError(f"No results for participant {args.participant} found in {experiment_dir}")

eval_save_dir = Path("./eval_results") / experiment_dir / f"{args.participant}"
eval_save_dir.mkdir(parents=True, exist_ok=True)

cache_dir = Path(".cache") / eval_save_dir
cache_dir.mkdir(parents=True, exist_ok=True)

stella_embedder = None

gen_0_human_dataset = json.loads((experiment_dir / "gen_0" / "log_sample_datasets.json").read_text(encoding="UTF-8"))['args']['human_dataset']

# extract the number of human and ai posts in the training set of each generation
def get_ai_human_n_posts(experiment_dir, gen_i, part):

    # how many human posts we add to each participant in generation_i
    human_n = json.loads((experiment_dir / f"gen_{gen_i}" / "log_sample_datasets.json").read_text(encoding="UTF-8"))['args']['per_participant_human_dataset_size']

    if gen_i == 0:
        gen_n = 0
    else:
        # n posts in training on generation gen_i
        # n generated posts
        # how many posts were generated by each participant in generation_i - 1
        gen_n = json.loads((experiment_dir / f"gen_{gen_i - 1}" / part / "log.json").read_text(encoding="UTF-8"))['args']['gen_unique_n']

    return gen_n, human_n

gen_n, human_n = get_ai_human_n_posts(experiment_dir, gen_i=1, part="part_0")
total_n = human_n + gen_n
human_ratio = human_n / total_n
ai_ratio = gen_n / total_n

print("Load and encode AI datasets")
all_datasets = []

n_generations = len(list(experiment_dir.glob(f"gen_[0-9]*/{args.participant}/full_output_dataset/data-00000-of-00001.arrow")))

print("N generations: ", n_generations)


# assert that all generations have the same ai_human ratio
for gen_i in range(1, n_generations):
    # we check that all generations generate the same number of posts
    gen_n_i, human_n_i = get_ai_human_n_posts(experiment_dir, gen_i=gen_i, part="part_0")
    assert gen_n == gen_n_i
    assert human_n == human_n_i

results = defaultdict(list)

# input datasets
################
input_datasets = [
    datasets.load_from_disk(str(experiment_dir / f"gen_{gen_i}" / f"{args.participant}/input_dataset")) \
    for gen_i in range(n_generations)
]

# input AI ratios
for inp_d in input_datasets:
    results["input_ai_ratio"].append(np.mean([t.startswith("AI") for t in inp_d['source']]))

# output datasets
#################
datasets_cache_path = cache_dir / f"datasets_emb_{args.emb}.pkl"
all_datasets_loaded = load_if_exists(datasets_cache_path)
if all_datasets_loaded is None:
    print(f"Loading datasets and adding embeddings")
    for gen_i in range(0, n_generations):
        print(f"Gen {gen_i}/{n_generations-1}")

        input_dataset = datasets.load_from_disk(str(experiment_dir / f"gen_{gen_i}" / f"{args.participant}/full_output_dataset"))

        with open(experiment_dir / f"gen_{gen_i}" / f"{args.participant}/log.json") as f:
            part_log = json.load(f)

        # take the first n regardless if they are unique or not (to make it fair)
        # otherwise diversity is artificially increased by oversampling
        gen_unique_n = part_log["args"]["gen_unique_n"]
        input_dataset = input_dataset.select(range(gen_unique_n))

        if args.emb:
            stella_embedder = StellaEmbedder() if stella_embedder is None else stella_embedder
            input_dataset = stella_embedder.add_embeddings(input_dataset)

        all_datasets.append(input_dataset)

    with open(datasets_cache_path, 'wb') as f:
        pickle.dump(all_datasets, f)
    print(f"Saved datasets to pickle: {datasets_cache_path}")
else:
    all_datasets = all_datasets_loaded

dataset_labels = [f"AI gen {i}" for i in range(len(all_datasets))]

# Show random samples
if args.n_samples_to_show > 0:
    for lab, d in zip(dataset_labels, datasets):
        print(f"{lab} random samples")
        samples = d.shuffle().select(range(args.n_samples_to_show))
        for sample in samples:
            print("\tSample:", sample['text'])

# Evaluate Metrics
results["dataset_labels"] = dataset_labels
results["gen_n"] = gen_n
results["human_n"] = human_n
results["human_ratio"] = human_ratio
results["ai_ratio"] = ai_ratio

print(f"AI ratio: {ai_ratio}")

ppl_model = 'Qwen/Qwen2.5-72B'
perplexity = None

for i, d in enumerate(all_datasets):
    print(f'datasets {i}/{len(all_datasets)}')

    # quick metrics
    results['ttr'].append([calculate_ttr(tx) for tx in d['text']])
    results['mttr'].append(calculate_ttr(" ".join(d['text'])))
    results['normalized_levenshtein_diversity'].append(compute_normalized_levenshtein_diversity(d["text"]))
    results['n_words'].append([num_words(tx) for tx in d['text']])
    results['n_unique_posts'].append(len(set(d['text'])))

    if args.emb:
        # embeddings
        emb_name = "stella"
        embs = np.array(d[f'{emb_name}_embeddings'])

        # posts are separated into buckets and diversity of buckets averaged to avoid the effect of sample size
        div_quant_size = 250

        # var diversities
        cache_pickle_path = cache_dir / f"{emb_name}_var_diversities_gen_{i}.pickle"
        var_diversity = load_if_exists(cache_pickle_path)
        if var_diversity is None:
            var_diversity = np.mean([
                compute_var_diveristy(e) for e in np.array_split(embs, len(embs) // div_quant_size, axis=0)
            ])

            with open(cache_pickle_path, 'wb') as f:
                pickle.dump(var_diversity, f)
            print(f"Saved var_diversities to: {cache_pickle_path}")
        results[f'var_diversity_{emb_name}'].append(var_diversity)

        # cos diversities
        cache_pickle_path = cache_dir / f'{emb_name}_cos_diversities_gen_{i}.pickle'
        cos_diversity = load_if_exists(cache_pickle_path)
        if cos_diversity is None:
            cos_diversity = np.mean([
                compute_cos_diveristy(e) for e in np.array_split(embs, len(embs) // div_quant_size, axis=0)
            ])

            with open(cache_pickle_path, 'wb') as f:
                pickle.dump(cos_diversity, f)
            print(f"Saved cos_diversities to: {cache_pickle_path}")

        results[f'cos_diversity_{emb_name}'].append(cos_diversity)

    # positivity
    cache_pickle_path = cache_dir / f'positivity_gen_{i}.pickle'
    positivity = load_if_exists(cache_pickle_path)
    if positivity is None:
        positivity = [get_positivity(tx) for tx in d['text']]
        with open(cache_pickle_path, 'wb') as f:
            pickle.dump(positivity, f)
        print(f"Saved positivity to: {cache_pickle_path}")

    results['positivity'].append(positivity)

    if args.tox:
        cache_pickle_path = cache_dir / f'toxicity_gen_{i}.pickle'
        toxicity = load_if_exists(cache_pickle_path)
        if toxicity is None:
            print("computing toxicity...")
            toxicity = get_toxicity_batch(d['text'], batch_size=1024)
            with open(cache_pickle_path, 'wb') as f:
                pickle.dump(toxicity, f)
            print(f"Saved toxicity to: {cache_pickle_path}")

        results['toxicity'].append(toxicity)

    if args.pol:
        cache_pickle_path = cache_dir / f'political_lean_gen_{i}.pickle'
        political_lean_and_scores = load_if_exists(cache_pickle_path)

        if political_lean_and_scores is None:
            print("computing political lean...")
            political_lean, political_lean_scores = get_political_lean_batch(d['text'])
            with open(political_lean, 'wb') as f:
                pickle.dump((political_lean, political_lean_scores), f)
            print(f"Saved political_lean to: {cache_pickle_path}")

        political_lean, political_lean_scores = political_lean_and_scores
        results['political_lean'].append(political_lean)
        results['political_lean_score'].append(political_lean_scores)

    if args.ce:

        cache_pickle_path = f'{cache_dir}/ce_ppl_{ppl_model.replace("/", "_")}_{i}.pickle'
        ce_ppl = load_if_exists(cache_pickle_path)

        if ce_ppl is None:
            if perplexity is None:
                print(f"Loading {ppl_model} perplexity")
                perplexity = Perplexity(ppl_model, model_args={"device_map": "auto", "torch_dtype": "auto"})

            response_template = "### RESPONSE\n"
            texts = [f"### INSTRUCTION\n{ins}\n{response_template}\n{tx}" for ins, tx in zip(d['instruction'], d['text'])]

            # bigger model
            ppl_res = perplexity.evaluate(
                texts, response_template=response_template,
                batch_size=4, add_start_token=False, max_length=1024, add_end_token=True
            )
            ppl = ppl_res["perplexities"]
            ce = ppl_res["cross_entropies"]

            with open(cache_pickle_path, 'wb') as f:
                pickle.dump((ce, ppl), f)
            print(f"Saved cross_entropy and perplexity ({ppl_model}) to: {cache_pickle_path}")

        else:
            ce, ppl = ce_ppl

        results[f'ppl_{ppl_model}'].append(ppl)
        results[f'ce_{ppl_model}'].append(ce)


results_path = eval_save_dir / 'results.json'
os.makedirs(results_path.parent, exist_ok=True)
with open(results_path, 'w') as results_file:
    json.dump(results, results_file, indent=6)

print(f'Metrics saved to: {results_path}')

if args.visualize_datasets:
    visualize_datasets(all_datasets, dataset_labels, experiment_dir)
